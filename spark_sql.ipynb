{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/01 15:56:47 WARN Utils: Your hostname, LAP-0285 resolves to a loopback address: 127.0.1.1; using 192.168.0.105 instead (on interface wlp0s20f3)\n",
      "24/10/01 15:56:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/01 15:56:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/10/01 15:56:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import \"file.csv\" into PySpark DataFrame\n",
      "Concatenating Name and Age columns:\n",
      "+-------------------+\n",
      "|           Name_Age|\n",
      "+-------------------+\n",
      "|     Keeley Bosco22|\n",
      "| Dr. Araceli Lang34|\n",
      "|    Terrell Boyle36|\n",
      "|Alessandro Barton21|\n",
      "|      Keven Purdy33|\n",
      "|   Jocelyn Abbott27|\n",
      "|       John Smith41|\n",
      "|  Samantha Garcia29|\n",
      "|      Jackie Chan56|\n",
      "|       Emma Stone32|\n",
      "+-------------------+\n",
      "\n",
      "Extracting the first letter from the City column:\n",
      "+------------+\n",
      "|First Letter|\n",
      "+------------+\n",
      "|           L|\n",
      "|           Y|\n",
      "|           P|\n",
      "|           S|\n",
      "|           P|\n",
      "|           N|\n",
      "|           S|\n",
      "|           L|\n",
      "|           B|\n",
      "|           L|\n",
      "+------------+\n",
      "\n",
      "Performing conditional logic on Age column:\n",
      "+-----------------+--------------------+---------+\n",
      "|             name|                city|Age Group|\n",
      "+-----------------+--------------------+---------+\n",
      "|     Keeley Bosco|     Lake Gladysberg|    Young|\n",
      "| Dr. Araceli Lang|         Yvettemouth|      Old|\n",
      "|    Terrell Boyle|     Port Reaganfort|      Old|\n",
      "|Alessandro Barton|         South Pearl|    Young|\n",
      "|      Keven Purdy|Port Marjolaineshire|      Old|\n",
      "|   Jocelyn Abbott|            New York|    Young|\n",
      "|       John Smith|       San Francisco|      Old|\n",
      "|  Samantha Garcia|         Los Angeles|    Young|\n",
      "|      Jackie Chan|             Beijing|      Old|\n",
      "|       Emma Stone|         Los Angeles|      Old|\n",
      "+-----------------+--------------------+---------+\n",
      "\n",
      "Counting the number of distinct colors in the Favorite Color column:\n",
      "+---------------+\n",
      "|Distinct Cities|\n",
      "+---------------+\n",
      "|              9|\n",
      "+---------------+\n",
      "\n",
      "Selecting the Name column:\n",
      "+-----------------+\n",
      "|             name|\n",
      "+-----------------+\n",
      "|     Keeley Bosco|\n",
      "| Dr. Araceli Lang|\n",
      "|    Terrell Boyle|\n",
      "|Alessandro Barton|\n",
      "|      Keven Purdy|\n",
      "|   Jocelyn Abbott|\n",
      "|       John Smith|\n",
      "|  Samantha Garcia|\n",
      "|      Jackie Chan|\n",
      "|       Emma Stone|\n",
      "+-----------------+\n",
      "\n",
      "Calculating the average age:\n",
      "+-----------+\n",
      "|Average Age|\n",
      "+-----------+\n",
      "|       33.1|\n",
      "+-----------+\n",
      "\n",
      "Calculating the average age by city:\n",
      "+--------------------+-------------------+\n",
      "|                city|Average Age by city|\n",
      "+--------------------+-------------------+\n",
      "|     Lake Gladysberg|               22.0|\n",
      "|             Beijing|               56.0|\n",
      "|         Los Angeles|               30.5|\n",
      "|       San Francisco|               41.0|\n",
      "|         Yvettemouth|               34.0|\n",
      "|Port Marjolaineshire|               33.0|\n",
      "|         South Pearl|               21.0|\n",
      "|     Port Reaganfort|               36.0|\n",
      "|            New York|               27.0|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, substring, when, countDistinct, col, avg\n",
    "spark = SparkSession.builder.appName(\"pyspark_sql\").getOrCreate()\n",
    "\n",
    "print(f'Import \"file.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Concatenating Name and Age columns:\")\n",
    "df.select(concat(\"name\", \"age\").alias(\"Name_Age\")).show()\n",
    "\n",
    "print(\"Extracting the first letter from the City column:\")\n",
    "df.select(substring(\"city\", 1, 1).alias(\"First Letter\")).show()\n",
    "\n",
    "print(\"Performing conditional logic on Age column:\")\n",
    "df.select(\"name\", \"city\", when(df[\"age\"] < 30, \"Young\").otherwise(\"Old\").alias(\"Age Group\")).show()\n",
    "\n",
    "print(\"Counting the number of distinct colors in the Favorite Color column:\")\n",
    "df.select(countDistinct(\"city\").alias(\"Distinct Cities\")).show()\n",
    "\n",
    "print(\"Selecting the Name column:\")\n",
    "df.select(col(\"name\")).show()\n",
    "\n",
    "print(\"Calculating the average age:\")\n",
    "df.agg(avg(col(\"age\")).alias(\"Average Age\")).show()\n",
    "\n",
    "print(\"Calculating the average age by city:\")\n",
    "df.groupBy(\"city\").agg(avg(\"age\").alias(\"Average Age by city\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-defined functions (UDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import \"file.csv\" into PySpark DataFrame\n",
      "Define a Python function\n",
      "Register the UDF\n",
      "Apply the registered UDF to a column in the DataFrame\n",
      "Display the final results of the DataFrame\n",
      "+---+-----------------+--------------------+--------------------+---+--------------------+\n",
      "| id|             name|               email|                city|age|    Capitalized City|\n",
      "+---+-----------------+--------------------+--------------------+---+--------------------+\n",
      "|  1|     Keeley Bosco|katlyn@jenkinsmag...|     Lake Gladysberg| 22|     LAKE GLADYSBERG|\n",
      "|  2| Dr. Araceli Lang|mavis_lehner@jaco...|         Yvettemouth| 34|         YVETTEMOUTH|\n",
      "|  3|    Terrell Boyle|augustine.conroy@...|     Port Reaganfort| 36|     PORT REAGANFORT|\n",
      "|  4|Alessandro Barton|sigurd.hudson@hod...|         South Pearl| 21|         SOUTH PEARL|\n",
      "|  5|      Keven Purdy|carter_zboncak@sc...|Port Marjolaineshire| 33|PORT MARJOLAINESHIRE|\n",
      "|  6|   Jocelyn Abbott|jocelyn.abbott@gm...|            New York| 27|            NEW YORK|\n",
      "|  7|       John Smith|john.smith@hotmai...|       San Francisco| 41|       SAN FRANCISCO|\n",
      "|  8|  Samantha Garcia|samantha.garcia@g...|         Los Angeles| 29|         LOS ANGELES|\n",
      "|  9|      Jackie Chan|jackie.chan@yahoo...|             Beijing| 56|             BEIJING|\n",
      "| 10|       Emma Stone|emma.stone@gmail.com|         Los Angeles| 32|         LOS ANGELES|\n",
      "+---+-----------------+--------------------+--------------------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(f'Import \"file.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Define a Python function\")\n",
    "def capitalize_string(s):\n",
    "    return s.upper()\n",
    "\n",
    "print(\"Register the UDF\")\n",
    "capitalize_udf = udf(capitalize_string, StringType())\n",
    "\n",
    "print(\"Apply the registered UDF to a column in the DataFrame\")\n",
    "df = df.withColumn(\"Capitalized City\", capitalize_udf(df[\"city\"]))\n",
    "\n",
    "print(\"Display the final results of the DataFrame\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running SQL queries programmatically in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import \"file.csv\" into PySpark DataFrame\n",
      "Define a Python function\n",
      "Register the UDF\n",
      "Apply the registered UDF to a column in the DataFrame\n",
      "Display the final results of the DataFrame\n",
      "+---+-----------------+--------------------+--------------------+---+--------------------+\n",
      "| id|             name|               email|                city|age|    Capitalized City|\n",
      "+---+-----------------+--------------------+--------------------+---+--------------------+\n",
      "|  1|     Keeley Bosco|katlyn@jenkinsmag...|     Lake Gladysberg| 22|     LAKE GLADYSBERG|\n",
      "|  2| Dr. Araceli Lang|mavis_lehner@jaco...|         Yvettemouth| 34|         YVETTEMOUTH|\n",
      "|  3|    Terrell Boyle|augustine.conroy@...|     Port Reaganfort| 36|     PORT REAGANFORT|\n",
      "|  4|Alessandro Barton|sigurd.hudson@hod...|         South Pearl| 21|         SOUTH PEARL|\n",
      "|  5|      Keven Purdy|carter_zboncak@sc...|Port Marjolaineshire| 33|PORT MARJOLAINESHIRE|\n",
      "|  6|   Jocelyn Abbott|jocelyn.abbott@gm...|            New York| 27|            NEW YORK|\n",
      "|  7|       John Smith|john.smith@hotmai...|       San Francisco| 41|       SAN FRANCISCO|\n",
      "|  8|  Samantha Garcia|samantha.garcia@g...|         Los Angeles| 29|         LOS ANGELES|\n",
      "|  9|      Jackie Chan|jackie.chan@yahoo...|             Beijing| 56|             BEIJING|\n",
      "| 10|       Emma Stone|emma.stone@gmail.com|         Los Angeles| 32|         LOS ANGELES|\n",
      "+---+-----------------+--------------------+--------------------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(f'Import \"file.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Define a Python function\")\n",
    "def capitalize_string(s):\n",
    "    return s.upper()\n",
    "\n",
    "print(\"Register the UDF\")\n",
    "capitalize_udf = udf(capitalize_string, StringType())\n",
    "\n",
    "print(\"Apply the registered UDF to a column in the DataFrame\")\n",
    "df = df.withColumn(\"Capitalized City\", capitalize_udf(df[\"city\"]))\n",
    "\n",
    "print(\"Display the final results of the DataFrame\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import \"file.csv\" into PySpark DataFrame\n",
      "Create a Global Temporary View\n",
      "Select city column from the temp view\n",
      "Display the contents\n",
      "+--------------------+\n",
      "|                city|\n",
      "+--------------------+\n",
      "|     Lake Gladysberg|\n",
      "|         Yvettemouth|\n",
      "|     Port Reaganfort|\n",
      "|         South Pearl|\n",
      "|Port Marjolaineshire|\n",
      "|            New York|\n",
      "|       San Francisco|\n",
      "|         Los Angeles|\n",
      "|             Beijing|\n",
      "|         Los Angeles|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate()\n",
    "\n",
    "print(f'Import \"file.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Create a Global Temporary View\")\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "print(\"Select city column from the temp view\")\n",
    "sqlDF = spark.sql(\"SELECT city FROM global_temp.people\")\n",
    "\n",
    "print(\"Display the contents\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import \"obesity.csv\" into PySpark DataFrame\n",
      "Create a Temporary View\n",
      "Select and display the first 5 rows and all columns from the temp view\n",
      "+---+---+------+------+------+----+-------------+\n",
      "| ID|Age|Gender|Height|Weight| BMI|        Label|\n",
      "+---+---+------+------+------+----+-------------+\n",
      "|  1| 25|  Male|   175|    80|25.3|Normal Weight|\n",
      "|  2| 30|Female|   160|    60|22.5|Normal Weight|\n",
      "|  3| 35|  Male|   180|    90|27.3|   Overweight|\n",
      "|  4| 40|Female|   150|    50|20.0|  Underweight|\n",
      "|  5| 45|  Male|   190|   100|31.2|        Obese|\n",
      "+---+---+------+------+------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Count the total number of rows in the DataFrame\n",
      "Total counts: 19\n",
      "Select and display all data points related to people with obesity\n",
      "+---+---+------+------+------+----+-----+\n",
      "| ID|Age|Gender|Height|Weight| BMI|Label|\n",
      "+---+---+------+------+------+----+-----+\n",
      "|  5| 45|  Male|   190|   100|31.2|Obese|\n",
      "|  7| 55|  Male|   200|   110|34.2|Obese|\n",
      "|  9| 65|  Male|   210|   120|37.2|Obese|\n",
      "| 17| 48|  Male|   200|   100|31.2|Obese|\n",
      "| 20| 58|  Male|   210|   110|34.2|Obese|\n",
      "+---+---+------+------+------+----+-----+\n",
      "\n",
      "Group people by label and display them the results\n",
      "+-------------+-----+\n",
      "|        Label|count|\n",
      "+-------------+-----+\n",
      "|Normal Weight|    5|\n",
      "|   Overweight|    2|\n",
      "|  Underweight|    7|\n",
      "|        Obese|    5|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark_sql\").getOrCreate()\n",
    "\n",
    "print(f'Import \"obesity.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"obesity.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Create a Temporary View\")\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "print(\"Select and display the first 5 rows and all columns from the temp view\")\n",
    "spark.sql(\"SELECT * FROM people\").show(5)\n",
    "\n",
    "print(\"Count the total number of rows in the DataFrame\")\n",
    "total_counts = spark.sql(\"SELECT COUNT(*) FROM people\").first()[0]\n",
    "print(f'Total counts: {total_counts}')\n",
    "\n",
    "print(\"Select and display all data points related to people with obesity\")\n",
    "spark.sql(\"SELECT * FROM people WHERE Label = 'Obese'\").show()\n",
    "\n",
    "print(\"Group people by label and display them the results\")\n",
    "spark.sql(\"SELECT Label, COUNT(*) AS count FROM people GROUP BY Label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data profiling and summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import \"obesity.csv\" into PySpark DataFrame\n",
      "Create a Temporary View\n",
      "Create a variable to capture column names\n",
      "Create an empty dictionary\n",
      "Iterate over each column in the DataFrame\n",
      "   Column  Distinct Count\n",
      "0      ID              19\n",
      "1     Age              19\n",
      "2  Gender               2\n",
      "3  Height              10\n",
      "4  Weight              11\n",
      "5     BMI              11\n",
      "6   Label               4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark_sql\").getOrCreate()\n",
    "\n",
    "print(f'Import \"obesity.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"obesity.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Create a Temporary View\")\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "print(\"Create a variable to capture column names\")\n",
    "columns = df.columns\n",
    "\n",
    "print(\"Create an empty dictionary\")\n",
    "distinct_counts = {}\n",
    "\n",
    "print(\"Iterate over each column in the DataFrame\")\n",
    "for column in columns:\n",
    "    query = f\"SELECT COUNT(DISTINCT `{column}`) FROM people\"\n",
    "    distinct_count = spark.sql(query).collect()[0][0]\n",
    "    distinct_counts[column] = distinct_count\n",
    "if distinct_counts:\n",
    "    pandas_df = pd.DataFrame.from_dict(distinct_counts, orient=\"index\", columns=[\"Distinct Count\"])\n",
    "    pandas_df.index.name = \"Column\"\n",
    "    pandas_df.reset_index(inplace=True)\n",
    "    print(pandas_df)\n",
    "else:\n",
    "    print(\"No distinct counts found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import \"obesity.csv\" into PySpark DataFrame\n",
      "Create a Temporary View\n",
      "Create a list containing new column names\n",
      "Create a SQL query\n",
      "Convert the output to Pandas\n",
      "Summary stats\n",
      "   summary  min(age)  max(age)  avg(age)  min(weight)  max(weight)  \\\n",
      "0  summary        18        70      43.0           20          120   \n",
      "\n",
      "   avg(weight)  min(height)  max(height)  avg(height)  \n",
      "0    71.052632          120          210   167.894737  \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark_sql\").getOrCreate()\n",
    "\n",
    "print(f'Import \"obesity.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"obesity.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Create a Temporary View\")\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "print(\"Create a list containing new column names\")\n",
    "numerical_columns = [\"age\", \"weight\", \"height\"]\n",
    "\n",
    "print(\"Create a SQL query\")\n",
    "sql_query = f\"SELECT 'summary' AS summary, {', '.join([f'min({column}), max({column}), avg({column})' for column in numerical_columns])} FROM people\"\n",
    "\n",
    "print(\"Convert the output to Pandas\")\n",
    "summary_stats = spark.sql(sql_query).toPandas()\n",
    "\n",
    "print(\"Summary stats\")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import \"obesity.csv\" into PySpark DataFrame\n",
      "Create a Temporary View\n",
      "Create a list containng new column names\n",
      "Iterate over columns and create \n",
      "Category counts for gender:\n",
      "Female 9\n",
      "Male 10\n",
      "Category counts for label:\n",
      "Normal Weight 5\n",
      "Overweight 2\n",
      "Underweight 7\n",
      "Obese 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark_sql\").getOrCreate()\n",
    "\n",
    "print(f'Import \"obesity.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"obesity.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Create a Temporary View\")\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "print(\"Create a list containng new column names\")\n",
    "categorical_columns = [\"gender\", \"label\"]\n",
    "\n",
    "print(\"Iterate over columns and create \")\n",
    "for column in categorical_columns:\n",
    "    query = f\"SELECT {column}, COUNT(*) AS count FROM people GROUP BY {column}\"\n",
    "    category_counts = spark.sql(query).collect()\n",
    "    print(f\"Category counts for {column}:\")\n",
    "    for row in category_counts:\n",
    "        print(row[column], row[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
