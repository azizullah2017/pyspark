{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/01 14:00:32 WARN Utils: Your hostname, LAP-0285 resolves to a loopback address: 127.0.1.1; using 192.168.0.105 instead (on interface wlp0s20f3)\n",
      "24/10/01 14:00:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/01 14:00:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a sample RDD\n",
      "Create a PySpark DataFrame from RDD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print the contents of the DataFrame\n",
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|  Alice|\n",
      "|  2|    Bob|\n",
      "|  3|Charlie|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark_sql\").getOrCreate()\n",
    "\n",
    "print(\"Create a sample RDD\")\n",
    "rdd = spark.sparkContext.parallelize([(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")])\n",
    "\n",
    "print(\"Create a PySpark DataFrame from RDD\")\n",
    "df = spark.createDataFrame(rdd, [\"id\", \"name\"])\n",
    "\n",
    "print(\"Print the contents of the DataFrame\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From external data sources#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import \"file.csv\" into PySpark DataFrame\n",
      "Print the contents of the PySpark DataFrame\n",
      "+-----------------+--------------------+--------------------+---+\n",
      "|             name|               email|                city|age|\n",
      "+-----------------+--------------------+--------------------+---+\n",
      "|     Keeley Bosco|katlyn@jenkinsmag...|     Lake Gladysberg| 22|\n",
      "| Dr. Araceli Lang|mavis_lehner@jaco...|         Yvettemouth| 34|\n",
      "|    Terrell Boyle|augustine.conroy@...|     Port Reaganfort| 36|\n",
      "|Alessandro Barton|sigurd.hudson@hod...|         South Pearl| 21|\n",
      "|      Keven Purdy|carter_zboncak@sc...|Port Marjolaineshire| 33|\n",
      "+-----------------+--------------------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark_sql\").getOrCreate()\n",
    "\n",
    "print(f'Import \"file.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Print the contents of the PySpark DataFrame\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From a pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a Python dictionary\n",
      "Create a Pandas DataFrame\n",
      "Convert Pandas DataFrame to PySpark DataFrame\n",
      "rows:  [Row(id=1, name='Alice', age=25), Row(id=2, name='Bob', age=30), Row(id=3, name='Charlie', age=35)]\n",
      "Print the contents of the PySpark DataFrame\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 35|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "spark = SparkSession.builder.appName(\"pyspark_sql\").getOrCreate()\n",
    "\n",
    "print(\"Create a Python dictionary\")\n",
    "data = {\"id\": [1, 2, 3], \"name\": [\"Alice\", \"Bob\", \"Charlie\"], \"age\": [25, 30, 35]}\n",
    "\n",
    "print(\"Create a Pandas DataFrame\")\n",
    "pandas_df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Convert Pandas DataFrame to PySpark DataFrame\")\n",
    "rows = [Row(**row) for row in pandas_df.to_dict(orient='records')]\n",
    "print(\"rows: \",rows)\n",
    "df = spark.createDataFrame(rows)\n",
    "\n",
    "print(\"Print the contents of the PySpark DataFrame\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame transformations\n",
    "\n",
    "select(): Selects columns from the DataFrame.\n",
    "\n",
    "filter(): Filters rows based on a condition.\n",
    "\n",
    "withColumn(): Adds or replaces a column in the DataFrame.\n",
    "\n",
    "groupBy(): Groups the DataFrame by specified columns.\n",
    "\n",
    "orderBy(): Sorts the DataFrame by specified columns.\n",
    "\n",
    "drop(): Drops specified columns from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import \"file.csv\" into PySpark DataFrame\n",
      "Showing the contents of the DataFrame:\n",
      "+-----------------+--------------------+\n",
      "|             name|                city|\n",
      "+-----------------+--------------------+\n",
      "|     Keeley Bosco|     Lake Gladysberg|\n",
      "| Dr. Araceli Lang|         Yvettemouth|\n",
      "|    Terrell Boyle|     Port Reaganfort|\n",
      "|Alessandro Barton|         South Pearl|\n",
      "|      Keven Purdy|Port Marjolaineshire|\n",
      "+-----------------+--------------------+\n",
      "\n",
      "Filtering for rows whose age is above 21:\n",
      "+----------------+--------------------+--------------------+---+\n",
      "|            name|               email|                city|age|\n",
      "+----------------+--------------------+--------------------+---+\n",
      "|    Keeley Bosco|katlyn@jenkinsmag...|     Lake Gladysberg| 22|\n",
      "|Dr. Araceli Lang|mavis_lehner@jaco...|         Yvettemouth| 34|\n",
      "|   Terrell Boyle|augustine.conroy@...|     Port Reaganfort| 36|\n",
      "|     Keven Purdy|carter_zboncak@sc...|Port Marjolaineshire| 33|\n",
      "+----------------+--------------------+--------------------+---+\n",
      "\n",
      "Creating a new column by subtracting 100 from the person's age:\n",
      "+-----------------+--------------------+--------------------+---+----------+\n",
      "|             name|               email|                city|age|new_column|\n",
      "+-----------------+--------------------+--------------------+---+----------+\n",
      "|     Keeley Bosco|katlyn@jenkinsmag...|     Lake Gladysberg| 22|        78|\n",
      "| Dr. Araceli Lang|mavis_lehner@jaco...|         Yvettemouth| 34|        66|\n",
      "|    Terrell Boyle|augustine.conroy@...|     Port Reaganfort| 36|        64|\n",
      "|Alessandro Barton|sigurd.hudson@hod...|         South Pearl| 21|        79|\n",
      "|      Keven Purdy|carter_zboncak@sc...|Port Marjolaineshire| 33|        67|\n",
      "+-----------------+--------------------+--------------------+---+----------+\n",
      "\n",
      "Grouping by city and then taking an average of age from that grouped city:\n",
      "+--------------------+--------+\n",
      "|                city|sum(age)|\n",
      "+--------------------+--------+\n",
      "|     Lake Gladysberg|      22|\n",
      "|         Yvettemouth|      34|\n",
      "|Port Marjolaineshire|      33|\n",
      "|         South Pearl|      21|\n",
      "|     Port Reaganfort|      36|\n",
      "+--------------------+--------+\n",
      "\n",
      "Sorting the DataFrame by age and printing the DataFrame:\n",
      "+-----------------+--------------------+--------------------+---+\n",
      "|             name|               email|                city|age|\n",
      "+-----------------+--------------------+--------------------+---+\n",
      "|Alessandro Barton|sigurd.hudson@hod...|         South Pearl| 21|\n",
      "|     Keeley Bosco|katlyn@jenkinsmag...|     Lake Gladysberg| 22|\n",
      "|      Keven Purdy|carter_zboncak@sc...|Port Marjolaineshire| 33|\n",
      "| Dr. Araceli Lang|mavis_lehner@jaco...|         Yvettemouth| 34|\n",
      "|    Terrell Boyle|augustine.conroy@...|     Port Reaganfort| 36|\n",
      "+-----------------+--------------------+--------------------+---+\n",
      "\n",
      "Removing email and city columns and printing the rest of the columns:\n",
      "+-----------------+---+\n",
      "|             name|age|\n",
      "+-----------------+---+\n",
      "|     Keeley Bosco| 22|\n",
      "| Dr. Araceli Lang| 34|\n",
      "|    Terrell Boyle| 36|\n",
      "|Alessandro Barton| 21|\n",
      "|      Keven Purdy| 33|\n",
      "+-----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameTransformations\").getOrCreate()\n",
    "\n",
    "print(f'Import \"file.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Showing the contents of the DataFrame:\")\n",
    "df.select(\"name\", \"city\").show()\n",
    "\n",
    "print(\"Filtering for rows whose age is above 21:\")\n",
    "df.filter(df[\"age\"] > 21).show()\n",
    "\n",
    "print(\"Creating a new column by subtracting 100 from the person's age:\")\n",
    "df.withColumn('new_column', 100 - df['age']).show()\n",
    "\n",
    "print(\"Grouping by city and then taking an average of age from that grouped city:\")\n",
    "df.groupBy(\"city\").agg({'age': 'sum'}).show()\n",
    "\n",
    "print(\"Sorting the DataFrame by age and printing the DataFrame:\")\n",
    "df.orderBy('age').show()\n",
    "\n",
    "print(\"Removing email and city columns and printing the rest of the columns:\")\n",
    "df.drop('email', 'city').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame actions\n",
    "\n",
    "show(): Displays the first n rows of the DataFrame.\n",
    "\n",
    "count(): Counts the number of rows in the DataFrame.\n",
    "\n",
    "collect(): Retrieves all the data in the DataFrame as a list.\n",
    "\n",
    "first(): Retrieves the first row of the DataFrame.\n",
    "\n",
    "take(n): Retrieves the first n rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import \"file.csv\" into PySpark DataFrame\n",
      "Printing the contents of the DataFrame:\n",
      "+-----------------+--------------------+--------------------+---+\n",
      "|             name|               email|                city|age|\n",
      "+-----------------+--------------------+--------------------+---+\n",
      "|     Keeley Bosco|katlyn@jenkinsmag...|     Lake Gladysberg| 22|\n",
      "| Dr. Araceli Lang|mavis_lehner@jaco...|         Yvettemouth| 34|\n",
      "|    Terrell Boyle|augustine.conroy@...|     Port Reaganfort| 36|\n",
      "|Alessandro Barton|sigurd.hudson@hod...|         South Pearl| 21|\n",
      "|      Keven Purdy|carter_zboncak@sc...|Port Marjolaineshire| 33|\n",
      "+-----------------+--------------------+--------------------+---+\n",
      "\n",
      "Showing the number of rows in the DataFrame:\n",
      "5\n",
      "Retrieving all the data in the DataFrame as a list instead of a table:\n",
      "[Row(name='Keeley Bosco', email='katlyn@jenkinsmaggio.net', city='Lake Gladysberg', age=22), Row(name='Dr. Araceli Lang', email='mavis_lehner@jacobi.name', city='Yvettemouth', age=34), Row(name='Terrell Boyle', email='augustine.conroy@keebler.name', city='Port Reaganfort', age=36), Row(name='Alessandro Barton', email='sigurd.hudson@hodkiewicz.net', city='South Pearl', age=21), Row(name='Keven Purdy', email='carter_zboncak@schmidtjenkins.info', city='Port Marjolaineshire', age=33)]\n",
      "Showing the first row of the DataFrame:\n",
      "Row(name='Keeley Bosco', email='katlyn@jenkinsmaggio.net', city='Lake Gladysberg', age=22)\n",
      "Showing the first two rows of the DataFrame:\n",
      "[Row(name='Keeley Bosco', email='katlyn@jenkinsmaggio.net', city='Lake Gladysberg', age=22), Row(name='Dr. Araceli Lang', email='mavis_lehner@jacobi.name', city='Yvettemouth', age=34)]\n",
      "Filtering for rows whose age is above 21:\n",
      "+----------------+--------------------+--------------------+---+\n",
      "|            name|               email|                city|age|\n",
      "+----------------+--------------------+--------------------+---+\n",
      "|    Keeley Bosco|katlyn@jenkinsmag...|     Lake Gladysberg| 22|\n",
      "|Dr. Araceli Lang|mavis_lehner@jaco...|         Yvettemouth| 34|\n",
      "|   Terrell Boyle|augustine.conroy@...|     Port Reaganfort| 36|\n",
      "|     Keven Purdy|carter_zboncak@sc...|Port Marjolaineshire| 33|\n",
      "+----------------+--------------------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(f'Import \"file.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Printing the contents of the DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "print(\"Showing the number of rows in the DataFrame:\")\n",
    "print(df.count())\n",
    "\n",
    "print(\"Retrieving all the data in the DataFrame as a list instead of a table:\")\n",
    "print(df.collect())\n",
    "\n",
    "print(\"Showing the first row of the DataFrame:\")\n",
    "print(df.first())\n",
    "\n",
    "print(\"Showing the first two rows of the DataFrame:\")\n",
    "print(df.take(2))\n",
    "\n",
    "print(\"Filtering for rows whose age is above 21:\")\n",
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import \"file.csv\" into PySpark DataFrame\n",
      "Import \"file2.csv\" into PySpark DataFrame\n",
      "Join dataframe 2 with DataFrame 1 using inner\n",
      "Showing the joined DataFrame using inner join:\n",
      "+---+-----------------+--------------------+--------------------+---+------------+\n",
      "| id|             name|               email|                city|age|credit_score|\n",
      "+---+-----------------+--------------------+--------------------+---+------------+\n",
      "|  1|     Keeley Bosco|katlyn@jenkinsmag...|     Lake Gladysberg| 22|         650|\n",
      "|  2| Dr. Araceli Lang|mavis_lehner@jaco...|         Yvettemouth| 34|         720|\n",
      "|  3|    Terrell Boyle|augustine.conroy@...|     Port Reaganfort| 36|         800|\n",
      "|  4|Alessandro Barton|sigurd.hudson@hod...|         South Pearl| 21|         600|\n",
      "|  5|      Keven Purdy|carter_zboncak@sc...|Port Marjolaineshire| 33|         750|\n",
      "+---+-----------------+--------------------+--------------------+---+------------+\n",
      "\n",
      "Join dataframe 2 with DataFrame 1 using outer\n",
      "Showing the joined DataFrame using outer join:\n",
      "+---+-----------------+--------------------+--------------------+---+------------+\n",
      "| id|             name|               email|                city|age|credit_score|\n",
      "+---+-----------------+--------------------+--------------------+---+------------+\n",
      "|  1|     Keeley Bosco|katlyn@jenkinsmag...|     Lake Gladysberg| 22|         650|\n",
      "|  2| Dr. Araceli Lang|mavis_lehner@jaco...|         Yvettemouth| 34|         720|\n",
      "|  3|    Terrell Boyle|augustine.conroy@...|     Port Reaganfort| 36|         800|\n",
      "|  4|Alessandro Barton|sigurd.hudson@hod...|         South Pearl| 21|         600|\n",
      "|  5|      Keven Purdy|carter_zboncak@sc...|Port Marjolaineshire| 33|         750|\n",
      "|  6|   Jocelyn Abbott|jocelyn.abbott@gm...|            New York| 27|        null|\n",
      "|  7|       John Smith|john.smith@hotmai...|       San Francisco| 41|        null|\n",
      "|  8|  Samantha Garcia|samantha.garcia@g...|         Los Angeles| 29|        null|\n",
      "|  9|      Jackie Chan|jackie.chan@yahoo...|             Beijing| 56|        null|\n",
      "| 10|       Emma Stone|emma.stone@gmail.com|         Los Angeles| 32|        null|\n",
      "+---+-----------------+--------------------+--------------------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate()\n",
    "\n",
    "print(f'Import \"file.csv\" into PySpark DataFrame')\n",
    "df1 = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(f'Import \"file2.csv\" into PySpark DataFrame')\n",
    "df2 = spark.read.csv(\"file2.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Join dataframe 2 with DataFrame 1 using inner\")\n",
    "joined_df = df1.join(df2, on=\"id\", how=\"inner\")\n",
    "\n",
    "print(\"Showing the joined DataFrame using inner join:\")\n",
    "joined_df.show()\n",
    "\n",
    "print(\"Join dataframe 2 with DataFrame 1 using outer\")\n",
    "joined_df = df1.join(df2, on=\"id\", how=\"outer\")\n",
    "\n",
    "print(\"Showing the joined DataFrame using outer join:\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import \"file.csv\" into PySpark DataFrame\n",
      "Printing the contents of the DataFrame\n",
      "+---+----------+--------+------+\n",
      "| id|department|employee|salary|\n",
      "+---+----------+--------+------+\n",
      "|  1|        IT|    John| 50000|\n",
      "|  2|        IT|    Jane| 55000|\n",
      "|  3|        IT|     Bob| 60000|\n",
      "|  4|        HR|   Alice| 65000|\n",
      "|  5|        HR|     Eve| 70000|\n",
      "|  6|        HR| Charles| 75000|\n",
      "|  7|   Finance|  Oliver| 80000|\n",
      "|  8|   Finance|   Ellen| 85000|\n",
      "|  9|   Finance|   Frank| 90000|\n",
      "| 10| Marketing|   Grace| 95000|\n",
      "| 11| Marketing|   Henry|100000|\n",
      "| 12| Marketing|     Ivy|105000|\n",
      "+---+----------+--------+------+\n",
      "\n",
      "Create a `WindowSpec` object\n",
      "Printing the output from Windows function\n",
      "+---+----------+--------+------+----------+\n",
      "| id|department|employee|salary|avg_salary|\n",
      "+---+----------+--------+------+----------+\n",
      "|  7|   Finance|  Oliver| 80000|   85000.0|\n",
      "|  8|   Finance|   Ellen| 85000|   85000.0|\n",
      "|  9|   Finance|   Frank| 90000|   85000.0|\n",
      "|  4|        HR|   Alice| 65000|   70000.0|\n",
      "|  5|        HR|     Eve| 70000|   70000.0|\n",
      "|  6|        HR| Charles| 75000|   70000.0|\n",
      "|  1|        IT|    John| 50000|   55000.0|\n",
      "|  2|        IT|    Jane| 55000|   55000.0|\n",
      "|  3|        IT|     Bob| 60000|   55000.0|\n",
      "| 10| Marketing|   Grace| 95000|  100000.0|\n",
      "| 11| Marketing|   Henry|100000|  100000.0|\n",
      "| 12| Marketing|     Ivy|105000|  100000.0|\n",
      "+---+----------+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "print(f'Import \"file.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"emp.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Printing the contents of the DataFrame\")\n",
    "df.show()\n",
    "\n",
    "print(\"Create a `WindowSpec` object\")\n",
    "windowSpec = Window.partitionBy('department')\n",
    "\n",
    "print(\"Printing the output from Windows function\")\n",
    "df.withColumn(\"avg_salary\", avg('salary').over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a `WindowSpec` object\n",
      "Import \"file.csv\" into PySpark DataFrame\n",
      "Assigning rank to each row:\n",
      "+---+----------+------------+------------+----+\n",
      "| id|sales_date|sales_person|sales_amount|rank|\n",
      "+---+----------+------------+------------+----+\n",
      "|  3|2022-01-02|       Alice|         750|   1|\n",
      "|  5|2022-01-03|       Alice|         800|   2|\n",
      "|  7|2022-01-04|       Alice|         900|   3|\n",
      "|  1|2022-01-01|       Alice|        1000|   4|\n",
      "|  9|2022-01-05|       Alice|        1500|   5|\n",
      "|  2|2022-01-01|         Bob|         500|   1|\n",
      "|  6|2022-01-03|         Bob|         600|   2|\n",
      "| 10|2022-01-05|         Bob|         800|   3|\n",
      "|  8|2022-01-04|         Bob|        1100|   4|\n",
      "|  4|2022-01-02|         Bob|        1200|   5|\n",
      "+---+----------+------------+------------+----+\n",
      "\n",
      "Assigning dense rank to each row:\n",
      "+---+----------+------------+------------+----------+\n",
      "| id|sales_date|sales_person|sales_amount|dense_rank|\n",
      "+---+----------+------------+------------+----------+\n",
      "|  3|2022-01-02|       Alice|         750|         1|\n",
      "|  5|2022-01-03|       Alice|         800|         2|\n",
      "|  7|2022-01-04|       Alice|         900|         3|\n",
      "|  1|2022-01-01|       Alice|        1000|         4|\n",
      "|  9|2022-01-05|       Alice|        1500|         5|\n",
      "|  2|2022-01-01|         Bob|         500|         1|\n",
      "|  6|2022-01-03|         Bob|         600|         2|\n",
      "| 10|2022-01-05|         Bob|         800|         3|\n",
      "|  8|2022-01-04|         Bob|        1100|         4|\n",
      "|  4|2022-01-02|         Bob|        1200|         5|\n",
      "+---+----------+------------+------------+----------+\n",
      "\n",
      "Assigning row number to each row:\n",
      "+---+----------+------------+------------+----------+\n",
      "| id|sales_date|sales_person|sales_amount|row_number|\n",
      "+---+----------+------------+------------+----------+\n",
      "|  3|2022-01-02|       Alice|         750|         1|\n",
      "|  5|2022-01-03|       Alice|         800|         2|\n",
      "|  7|2022-01-04|       Alice|         900|         3|\n",
      "|  1|2022-01-01|       Alice|        1000|         4|\n",
      "|  9|2022-01-05|       Alice|        1500|         5|\n",
      "|  2|2022-01-01|         Bob|         500|         1|\n",
      "|  6|2022-01-03|         Bob|         600|         2|\n",
      "| 10|2022-01-05|         Bob|         800|         3|\n",
      "|  8|2022-01-04|         Bob|        1100|         4|\n",
      "|  4|2022-01-02|         Bob|        1200|         5|\n",
      "+---+----------+------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, dense_rank, row_number\n",
    "\n",
    "print(\"Create a `WindowSpec` object\")\n",
    "windowSpec = Window.partitionBy('sales_person').orderBy('sales_amount')\n",
    "\n",
    "print(f'Import \"file.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"sales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Assigning rank to each row:\")\n",
    "df.withColumn('rank', rank().over(windowSpec)).show()\n",
    "\n",
    "print(\"Assigning dense rank to each row:\")\n",
    "df.withColumn('dense_rank', dense_rank().over(windowSpec)).show()\n",
    "\n",
    "print(\"Assigning row number to each row:\")\n",
    "df.withColumn('row_number', row_number().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytic functions\n",
    "Analytic functions compute a value for each input row based on a group of related rows. Commonly used analytic functions include lead(), lag(), and ntile()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a `WindowSpec` object\n",
      "Import \"file.csv\" into PySpark DataFrame\n",
      "Next month's sales for each category:\n",
      "+---+----------+------------+------------+----------------+\n",
      "| id|sales_date|sales_person|sales_amount|next_month_sales|\n",
      "+---+----------+------------+------------+----------------+\n",
      "|  3|2022-01-02|       Alice|         750|             800|\n",
      "|  5|2022-01-03|       Alice|         800|             900|\n",
      "|  7|2022-01-04|       Alice|         900|            1000|\n",
      "|  1|2022-01-01|       Alice|        1000|            1500|\n",
      "|  9|2022-01-05|       Alice|        1500|            null|\n",
      "|  2|2022-01-01|         Bob|         500|             600|\n",
      "|  6|2022-01-03|         Bob|         600|             800|\n",
      "| 10|2022-01-05|         Bob|         800|            1100|\n",
      "|  8|2022-01-04|         Bob|        1100|            1200|\n",
      "|  4|2022-01-02|         Bob|        1200|            null|\n",
      "+---+----------+------------+------------+----------------+\n",
      "\n",
      "Previous month's sales for each category:\n",
      "+---+----------+------------+------------+----------------+\n",
      "| id|sales_date|sales_person|sales_amount|prev_month_sales|\n",
      "+---+----------+------------+------------+----------------+\n",
      "|  3|2022-01-02|       Alice|         750|            null|\n",
      "|  5|2022-01-03|       Alice|         800|             750|\n",
      "|  7|2022-01-04|       Alice|         900|             800|\n",
      "|  1|2022-01-01|       Alice|        1000|             900|\n",
      "|  9|2022-01-05|       Alice|        1500|            1000|\n",
      "|  2|2022-01-01|         Bob|         500|            null|\n",
      "|  6|2022-01-03|         Bob|         600|             500|\n",
      "| 10|2022-01-05|         Bob|         800|             600|\n",
      "|  8|2022-01-04|         Bob|        1100|             800|\n",
      "|  4|2022-01-02|         Bob|        1200|            1100|\n",
      "+---+----------+------------+------------+----------------+\n",
      "\n",
      "Quartiles of sales for each category:\n",
      "+---+----------+------------+------------+--------+\n",
      "| id|sales_date|sales_person|sales_amount|quartile|\n",
      "+---+----------+------------+------------+--------+\n",
      "|  3|2022-01-02|       Alice|         750|       1|\n",
      "|  5|2022-01-03|       Alice|         800|       1|\n",
      "|  7|2022-01-04|       Alice|         900|       2|\n",
      "|  1|2022-01-01|       Alice|        1000|       3|\n",
      "|  9|2022-01-05|       Alice|        1500|       4|\n",
      "|  2|2022-01-01|         Bob|         500|       1|\n",
      "|  6|2022-01-03|         Bob|         600|       1|\n",
      "| 10|2022-01-05|         Bob|         800|       2|\n",
      "|  8|2022-01-04|         Bob|        1100|       3|\n",
      "|  4|2022-01-02|         Bob|        1200|       4|\n",
      "+---+----------+------------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead, lag, ntile\n",
    "\n",
    "print(\"Create a `WindowSpec` object\")\n",
    "windowSpec = Window.partitionBy('sales_person').orderBy('sales_amount')\n",
    "\n",
    "print(f'Import \"file.csv\" into PySpark DataFrame')\n",
    "df = spark.read.csv(\"sales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Next month's sales for each category:\")\n",
    "df.withColumn('next_month_sales', lead('sales_amount', 1).over(windowSpec)).show()\n",
    "\n",
    "print(\"Previous month's sales for each category:\")\n",
    "df.withColumn('prev_month_sales', lag('sales_amount', 1).over(windowSpec)).show()\n",
    "\n",
    "print(\"Quartiles of sales for each category:\")\n",
    "df.withColumn('quartile', ntile(4).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
